{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.3\n",
    "epochs = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        self.weights = np.random.uniform(-1, 1, input_size)\n",
    "        self.bias = np.random.uniform()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x \n",
    "        z = np.dot(self.input, self.weights) + self.bias\n",
    "        self.output = sigmoid(z)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, loss):\n",
    "        delta = loss * sigmoid_derivative(self.output)\n",
    "        self.weights -= delta * self.input * lr\n",
    "        self.bias -= delta * lr\n",
    "        return self.weights * delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.hidden_neuron1 = Neuron(2)\n",
    "        self.hidden_neuron2 = Neuron(2)\n",
    "        self.output_neuron = Neuron(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.hidden_output1 = self.hidden_neuron1.forward(x)\n",
    "        self.hidden_output2 = self.hidden_neuron2.forward(x)\n",
    "        hidden_outputs = np.array([self.hidden_output1, self.hidden_output2])\n",
    "        self.output = self.output_neuron.forward(hidden_outputs)\n",
    "        return self.output\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def backward(self, loss):\n",
    "        delta = loss * sigmoid_derivative(self.output)\n",
    "        delta_hidden = self.output_neuron.backward(delta)\n",
    "        self.hidden_neuron1.backward(delta_hidden[0])\n",
    "        self.hidden_neuron2.backward(delta_hidden[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_pred, y_true):\n",
    "    return y_pred - y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    (np.array([0, 0]), 0),\n",
    "    (np.array([0, 1]), 1),\n",
    "    (np.array([1, 0]), 1),\n",
    "    (np.array([1, 1]), 0),\n",
    "]\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  Loss = 0.639499539586948\n",
      "Epoch 500:  Loss = 0.4779206080740938\n",
      "Epoch 1000:  Loss = 0.48217164550463626\n",
      "Epoch 1500:  Loss = 0.48476688913103033\n",
      "Epoch 2000:  Loss = 0.4874166971677525\n",
      "Epoch 2500:  Loss = 0.4917462934925413\n",
      "Epoch 3000:  Loss = 0.4935715612814385\n",
      "Epoch 3500:  Loss = 0.4768977168514987\n",
      "Epoch 4000:  Loss = 0.41101443211210753\n",
      "Epoch 4500:  Loss = 0.3261209179257415\n",
      "Epoch 5000:  Loss = 0.26322270661640484\n",
      "Epoch 5500:  Loss = 0.22215700880583736\n",
      "Epoch 6000:  Loss = 0.19454338452739636\n",
      "Epoch 6500:  Loss = 0.17490957254765155\n",
      "Epoch 7000:  Loss = 0.1602310482139061\n",
      "Epoch 7500:  Loss = 0.14880488476969417\n",
      "Epoch 8000:  Loss = 0.13962164989032058\n",
      "Epoch 8500:  Loss = 0.13205070195337146\n",
      "Epoch 9000:  Loss = 0.12567920700797874\n",
      "Epoch 9500:  Loss = 0.120225869852728\n",
      "Epoch 10000:  Loss = 0.11549237686552473\n",
      "Epoch 10500:  Loss = 0.11133479096595841\n",
      "Epoch 11000:  Loss = 0.1076460009848987\n",
      "Epoch 11500:  Loss = 0.10434456044376304\n",
      "Epoch 12000:  Loss = 0.10136736223982235\n",
      "Epoch 12500:  Loss = 0.09866469537091632\n",
      "Epoch 13000:  Loss = 0.09619682600654757\n",
      "Epoch 13500:  Loss = 0.09393158053107133\n",
      "Epoch 14000:  Loss = 0.09184260317339585\n",
      "Epoch 14500:  Loss = 0.08990807769123192\n",
      "Epoch 15000:  Loss = 0.08810977454093491\n",
      "Epoch 15500:  Loss = 0.08643233039017502\n",
      "Epoch 16000:  Loss = 0.08486269615676538\n",
      "Epoch 16500:  Loss = 0.08338970908092466\n",
      "Epoch 17000:  Loss = 0.08200375731209383\n",
      "Epoch 17500:  Loss = 0.08069651435284132\n",
      "Epoch 18000:  Loss = 0.07946072685131111\n",
      "Epoch 18500:  Loss = 0.0782900435631819\n",
      "Epoch 19000:  Loss = 0.07717887639387361\n",
      "Epoch 19500:  Loss = 0.0761222866646528\n",
      "Epoch 20000:  Loss = 0.07511589137876398\n",
      "Epoch 20500:  Loss = 0.0741557854704299\n",
      "Epoch 21000:  Loss = 0.07323847692034754\n",
      "Epoch 21500:  Loss = 0.07236083230051814\n",
      "Epoch 22000:  Loss = 0.07152003082764419\n",
      "Epoch 22500:  Loss = 0.07071352540044344\n",
      "Epoch 23000:  Loss = 0.06993900940236147\n",
      "Epoch 23500:  Loss = 0.06919438828966605\n",
      "Epoch 24000:  Loss = 0.068477755171834\n",
      "Epoch 24500:  Loss = 0.06778736973881448\n",
      "Epoch 25000:  Loss = 0.0671216400070071\n",
      "Epoch 25500:  Loss = 0.06647910644957682\n",
      "Epoch 26000:  Loss = 0.06585842815202185\n",
      "Epoch 26500:  Loss = 0.06525837069485327\n",
      "Epoch 27000:  Loss = 0.06467779551467479\n",
      "Epoch 27500:  Loss = 0.06411565053534442\n",
      "Epoch 28000:  Loss = 0.06357096189399265\n",
      "Epoch 28500:  Loss = 0.06304282661398632\n",
      "Epoch 29000:  Loss = 0.06253040609945487\n",
      "Epoch 29500:  Loss = 0.06203292034478533\n",
      "Epoch 30000:  Loss = 0.06154964276808593\n",
      "Epoch 30500:  Loss = 0.061079895590757396\n",
      "Epoch 31000:  Loss = 0.0606230456962505\n",
      "Epoch 31500:  Loss = 0.060178500910424426\n",
      "Epoch 32000:  Loss = 0.05974570665369043\n",
      "Epoch 32500:  Loss = 0.059324142921866545\n",
      "Epoch 33000:  Loss = 0.05891332155824755\n",
      "Epoch 33500:  Loss = 0.05851278378428978\n",
      "Epoch 34000:  Loss = 0.05812209796044747\n",
      "Epoch 34500:  Loss = 0.05774085755220917\n",
      "Epoch 35000:  Loss = 0.05736867927949177\n",
      "Epoch 35500:  Loss = 0.057005201430191284\n",
      "Epoch 36000:  Loss = 0.05665008232091911\n",
      "Epoch 36500:  Loss = 0.05630299889002254\n",
      "Epoch 37000:  Loss = 0.05596364540961699\n",
      "Epoch 37500:  Loss = 0.05563173230494808\n",
      "Epoch 38000:  Loss = 0.055306985070656366\n",
      "Epoch 38500:  Loss = 0.054989143274677035\n",
      "Epoch 39000:  Loss = 0.05467795964151965\n",
      "Epoch 39500:  Loss = 0.0543731992075812\n",
      "Epoch 40000:  Loss = 0.054074638541853445\n",
      "Epoch 40500:  Loss = 0.05378206502616292\n",
      "Epoch 41000:  Loss = 0.05349527618961392\n",
      "Epoch 41500:  Loss = 0.053214079092487855\n",
      "Epoch 42000:  Loss = 0.05293828975530994\n",
      "Epoch 42500:  Loss = 0.05266773262921822\n",
      "Epoch 43000:  Loss = 0.052402240104142454\n",
      "Epoch 43500:  Loss = 0.05214165205164696\n",
      "Epoch 44000:  Loss = 0.051885815399598585\n",
      "Epoch 44500:  Loss = 0.05163458373604504\n",
      "Epoch 45000:  Loss = 0.05138781693998023\n",
      "Epoch 45500:  Loss = 0.05114538083687329\n",
      "Epoch 46000:  Loss = 0.050907146876985\n",
      "Epoch 46500:  Loss = 0.050672991834747216\n",
      "Epoch 47000:  Loss = 0.05044279752758216\n",
      "Epoch 47500:  Loss = 0.050216450552682444\n",
      "Epoch 48000:  Loss = 0.04999384204041727\n",
      "Epoch 48500:  Loss = 0.049774867423146886\n",
      "Epoch 49000:  Loss = 0.049559426218293415\n",
      "Epoch 49500:  Loss = 0.04934742182467393\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for x, label in inputs: \n",
    "        y_pred = model.forward(x)\n",
    "        err = loss_fn(y_pred, label)\n",
    "        model.backward(err)\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}:  Loss = {err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.040915704113223775\n",
      "0.9573358274885975\n",
      "0.9573338137704753\n",
      "0.04913552952479604\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([0, 0]))\n",
    "print(model.predict([0, 1]))\n",
    "print(model.predict([1, 0]))\n",
    "print(model.predict([1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
